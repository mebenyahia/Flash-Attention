model:
  d_model: 512
  n_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  d_ff: 2048
  dropout: 0.1
  label_smoothing: 0.1
  max_position_embeddings: 512

training:
  batch_size: 50
  learning_rate: 0.0001
  warmup_steps: 2000
  max_epochs: 200
  mixed_precision: true
  gradient_accumulation_steps: 2
  device: "cuda"  # or "cpu"

data:
  dataset_name: "wmt14"
  dataset_config: "de-en"
  subset_percentage: 1
  src_lang: "en"
  tgt_lang: "de"
  train_split: "train"
  valid_split: "validation"
  test_split: "test"
  vocab_size: 8000
  combined_corpus_file: "data/tokenized/combined.txt"
  vocab_prefix: "data/tokenized/vocab"
  train_bpe_src: "data/tokenized/train.en.bpe"
  train_bpe_tgt: "data/tokenized/train.de.bpe"
  valid_bpe_src: "data/tokenized/valid.en.bpe"
  valid_bpe_tgt: "data/tokenized/valid.de.bpe"
  test_bpe_src: "data/tokenized/test.en.bpe"
  test_bpe_tgt: "data/tokenized/test.de.bpe"

beam_search:
  beam_size: 5
  max_length: 60
  length_penalty: 1.0

logging:
  log_interval: 50
  save_checkpoint_interval: 500
  checkpoint_dir: "checkpoints"
